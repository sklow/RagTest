{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Hybrid Search - GPU Model Comparison\n",
    "\n",
    "Dense + Sparse ハイブリッド検索のマルチモデル比較（GPU版）\n",
    "\n",
    "**ランタイム → ランタイムのタイプを変更 → T4 GPU** を選択してから実行してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pymupdf sentence-transformers chromadb torch numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: GPUが検出されません。ランタイム→ランタイムのタイプを変更→T4 GPUを選択してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDFアップロード\n",
    "\n",
    "`TMC4361A_datasheet_rev1.26_01.pdf` をアップロードしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "PDF_PATH = \"TMC4361A_datasheet_rev1.26_01.pdf\"\n",
    "\n",
    "if not os.path.exists(PDF_PATH):\n",
    "    print(\"PDFファイルをアップロードしてください...\")\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        PDF_PATH = list(uploaded.keys())[0]\n",
    "        print(f\"Uploaded: {PDF_PATH}\")\n",
    "else:\n",
    "    print(f\"PDF already exists: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. モジュール定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def split_into_chunks(self, text: str) -> List[Dict[str, str]]:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_length = len(text)\n",
    "        chunk_id = 0\n",
    "        while start < text_length:\n",
    "            end = start + self.chunk_size\n",
    "            if end < text_length:\n",
    "                next_newline = text.find('\\n', end)\n",
    "                if next_newline != -1 and next_newline - end < 100:\n",
    "                    end = next_newline\n",
    "            chunk_text = text[start:end].strip()\n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    'id': f'chunk_{chunk_id}',\n",
    "                    'text': chunk_text,\n",
    "                    'start_pos': start,\n",
    "                    'end_pos': end\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            start = end - self.chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "    def process_pdf(self, pdf_path: str) -> List[Dict[str, str]]:\n",
    "        print(f\"Processing PDF: {pdf_path}\")\n",
    "        raw_text = self.extract_text_from_pdf(pdf_path)\n",
    "        print(f\"Extracted {len(raw_text)} characters\")\n",
    "        cleaned_text = self.clean_text(raw_text)\n",
    "        print(f\"Cleaned text: {len(cleaned_text)} characters\")\n",
    "        chunks = self.split_into_chunks(cleaned_text)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class HybridRAGSystem:\n",
    "    def __init__(self, collection_name: str = \"rag_documents\",\n",
    "                 model_name: str = \"all-MiniLM-L6-v2\",\n",
    "                 device: str = None):\n",
    "        print(f\"Initializing RAG system with model: {model_name}\")\n",
    "\n",
    "        # デバイス自動検出\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = device\n",
    "        print(f\"  Device: {device}\")\n",
    "\n",
    "        self.dense_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            anonymized_telemetry=False,\n",
    "            allow_reset=True\n",
    "        ))\n",
    "        try:\n",
    "            self.client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "        self.collection = self.client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        print(f\"  Collection '{collection_name}' initialized\")\n",
    "\n",
    "    def compute_dense_embedding(self, text: str) -> List[float]:\n",
    "        embedding = self.dense_model.encode(text, convert_to_numpy=True)\n",
    "        return embedding.tolist()\n",
    "\n",
    "    def compute_dense_embeddings_batch(self, texts: List[str], batch_size: int = 64) -> List[List[float]]:\n",
    "        \"\"\"バッチでDense embeddingを計算（GPU活用）\"\"\"\n",
    "        embeddings = self.dense_model.encode(texts, batch_size=batch_size,\n",
    "                                             show_progress_bar=True,\n",
    "                                             convert_to_numpy=True)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def compute_sparse_embedding(self, text: str) -> Dict[str, float]:\n",
    "        tokens = text.lower().split()\n",
    "        tf = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "        total_tokens = len(tokens)\n",
    "        return {token: count / total_tokens for token, count in tf.items()}\n",
    "\n",
    "    def add_documents(self, chunks: List[Dict[str, str]], batch_size: int = 64):\n",
    "        \"\"\"ドキュメントをバッチ処理でベクトルDBに追加（GPU最適化）\"\"\"\n",
    "        print(f\"Adding {len(chunks)} documents (batch_size={batch_size})...\")\n",
    "\n",
    "        texts = [chunk['text'] for chunk in chunks]\n",
    "        embeddings = self.compute_dense_embeddings_batch(texts, batch_size=batch_size)\n",
    "\n",
    "        ids = [chunk['id'] for chunk in chunks]\n",
    "        documents = texts\n",
    "        metadatas = [{\n",
    "            'start_pos': chunk['start_pos'],\n",
    "            'end_pos': chunk['end_pos'],\n",
    "            'sparse_tokens': len(self.compute_sparse_embedding(chunk['text'])),\n",
    "        } for chunk in chunks]\n",
    "\n",
    "        # ChromaDBは大きなバッチを分割して追加\n",
    "        chroma_batch = 500\n",
    "        for i in range(0, len(ids), chroma_batch):\n",
    "            end = min(i + chroma_batch, len(ids))\n",
    "            self.collection.add(\n",
    "                ids=ids[i:end],\n",
    "                embeddings=embeddings[i:end],\n",
    "                documents=documents[i:end],\n",
    "                metadatas=metadatas[i:end]\n",
    "            )\n",
    "\n",
    "        print(f\"Successfully added {len(chunks)} documents\")\n",
    "\n",
    "    @staticmethod\n",
    "    def min_max_normalize(scores: List[float]) -> np.ndarray:\n",
    "        scores = np.array(scores)\n",
    "        if len(scores) == 0:\n",
    "            return scores\n",
    "        min_s, max_s = scores.min(), scores.max()\n",
    "        if max_s - min_s == 0:\n",
    "            return np.ones_like(scores)\n",
    "        return (scores - min_s) / (max_s - min_s)\n",
    "\n",
    "    def dense_search(self, query: str, top_k: int = 10) -> List[Tuple[str, float, str]]:\n",
    "        query_embedding = self.compute_dense_embedding(query)\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        formatted = []\n",
    "        if results['ids'] and len(results['ids']) > 0:\n",
    "            for i in range(len(results['ids'][0])):\n",
    "                score = 1 - results['distances'][0][i]\n",
    "                formatted.append((results['ids'][0][i], score, results['documents'][0][i]))\n",
    "        return formatted\n",
    "\n",
    "    def sparse_search(self, query: str, documents: List[Dict], top_k: int = 10) -> List[Tuple[str, float]]:\n",
    "        query_tokens = set(query.lower().split())\n",
    "        scores = []\n",
    "        for doc in documents:\n",
    "            doc_token_set = set(doc['text'].lower().split())\n",
    "            intersection = query_tokens & doc_token_set\n",
    "            union = query_tokens | doc_token_set\n",
    "            score = len(intersection) / len(union) if len(union) > 0 else 0.0\n",
    "            scores.append((doc['id'], score))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:top_k]\n",
    "\n",
    "    def hybrid_search(self, query: str, alpha: float = 0.5, top_k: int = 10) -> List[Dict]:\n",
    "        dense_results = self.dense_search(query, top_k=top_k * 2)\n",
    "        all_docs_result = self.collection.get()\n",
    "        all_docs = [\n",
    "            {'id': all_docs_result['ids'][i], 'text': all_docs_result['documents'][i]}\n",
    "            for i in range(len(all_docs_result['ids']))\n",
    "        ]\n",
    "        sparse_results = self.sparse_search(query, all_docs, top_k=top_k * 2)\n",
    "\n",
    "        doc_scores = {}\n",
    "        dense_ids = [r[0] for r in dense_results]\n",
    "        dense_scores = [r[1] for r in dense_results]\n",
    "        dense_normalized = self.min_max_normalize(dense_scores)\n",
    "        for i, doc_id in enumerate(dense_ids):\n",
    "            doc_scores[doc_id] = {'dense': dense_normalized[i], 'sparse': 0.0}\n",
    "\n",
    "        sparse_ids = [r[0] for r in sparse_results]\n",
    "        sparse_scores_raw = [r[1] for r in sparse_results]\n",
    "        sparse_normalized = self.min_max_normalize(sparse_scores_raw)\n",
    "        for i, doc_id in enumerate(sparse_ids):\n",
    "            if doc_id not in doc_scores:\n",
    "                doc_scores[doc_id] = {'dense': 0.0, 'sparse': sparse_normalized[i]}\n",
    "            else:\n",
    "                doc_scores[doc_id]['sparse'] = sparse_normalized[i]\n",
    "\n",
    "        final_scores = []\n",
    "        for doc_id, scores in doc_scores.items():\n",
    "            combined = alpha * scores['dense'] + (1 - alpha) * scores['sparse']\n",
    "            final_scores.append((doc_id, combined, scores['dense'], scores['sparse']))\n",
    "        final_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        results = []\n",
    "        for doc_id, combined_score, dense_score, sparse_score in final_scores[:top_k]:\n",
    "            doc_result = self.collection.get(ids=[doc_id])\n",
    "            if doc_result['documents']:\n",
    "                results.append({\n",
    "                    'id': doc_id,\n",
    "                    'score': combined_score,\n",
    "                    'dense_score': dense_score,\n",
    "                    'sparse_score': sparse_score,\n",
    "                    'text': doc_result['documents'][0]\n",
    "                })\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CASES = [\n",
    "    # カテゴリ1: 具体的数値検索 (Factual/Numeric)\n",
    "    {\"query\": \"What is the supply voltage range for TMC4361A?\",\n",
    "     \"category\": \"factual_numeric\", \"difficulty\": \"easy\",\n",
    "     \"expected_keywords\": [\"voltage\", \"VCC\", \"3.3\", \"5\", \"supply\"]},\n",
    "    {\"query\": \"What is the maximum SPI clock frequency?\",\n",
    "     \"category\": \"factual_numeric\", \"difficulty\": \"medium\",\n",
    "     \"expected_keywords\": [\"SPI\", \"clock\", \"frequency\", \"MHz\"]},\n",
    "    {\"query\": \"Operating temperature range\",\n",
    "     \"category\": \"factual_numeric\", \"difficulty\": \"easy\",\n",
    "     \"expected_keywords\": [\"temperature\", \"\\u00b0C\", \"-40\", \"125\", \"operating\"]},\n",
    "    # カテゴリ2: 概念的質問 (Conceptual)\n",
    "    {\"query\": \"How does the S-shaped ramp generator work?\",\n",
    "     \"category\": \"conceptual\", \"difficulty\": \"medium\",\n",
    "     \"expected_keywords\": [\"ramp\", \"velocity\", \"acceleration\", \"profile\", \"S-shaped\"]},\n",
    "    {\"query\": \"What is the purpose of closed-loop operation?\",\n",
    "     \"category\": \"conceptual\", \"difficulty\": \"medium\",\n",
    "     \"expected_keywords\": [\"closed-loop\", \"encoder\", \"feedback\", \"position\", \"control\"]},\n",
    "    {\"query\": \"How does the encoder interface function?\",\n",
    "     \"category\": \"conceptual\", \"difficulty\": \"medium\",\n",
    "     \"expected_keywords\": [\"encoder\", \"interface\", \"incremental\", \"position\", \"ABN\"]},\n",
    "    # カテゴリ3: 専門用語・略語 (Technical Terms)\n",
    "    {\"query\": \"XTARGET register description\",\n",
    "     \"category\": \"technical_terms\", \"difficulty\": \"easy\",\n",
    "     \"expected_keywords\": [\"XTARGET\", \"register\", \"target\", \"position\"]},\n",
    "    {\"query\": \"ChopSync feature explanation\",\n",
    "     \"category\": \"technical_terms\", \"difficulty\": \"hard\",\n",
    "     \"expected_keywords\": [\"ChopSync\", \"chopper\", \"synchronization\"]},\n",
    "    {\"query\": \"SixPoint ramp mode\",\n",
    "     \"category\": \"technical_terms\", \"difficulty\": \"medium\",\n",
    "     \"expected_keywords\": [\"SixPoint\", \"ramp\", \"motion\", \"profile\"]},\n",
    "    # カテゴリ4: 複合条件検索 (Multi-aspect)\n",
    "    {\"query\": \"SPI communication protocol for motor speed control\",\n",
    "     \"category\": \"multi_aspect\", \"difficulty\": \"hard\",\n",
    "     \"expected_keywords\": [\"SPI\", \"speed\", \"control\", \"register\", \"velocity\"]},\n",
    "    {\"query\": \"Encoder feedback for position tracking accuracy\",\n",
    "     \"category\": \"multi_aspect\", \"difficulty\": \"hard\",\n",
    "     \"expected_keywords\": [\"encoder\", \"position\", \"feedback\", \"accuracy\", \"deviation\"]},\n",
    "    {\"query\": \"Reference switch configuration and homing procedure\",\n",
    "     \"category\": \"multi_aspect\", \"difficulty\": \"hard\",\n",
    "     \"expected_keywords\": [\"reference\", \"switch\", \"homing\", \"configuration\"]},\n",
    "    # カテゴリ5: 意味的類似検索 (Semantic Similarity)\n",
    "    {\"query\": \"How to make the motor move smoothly without vibration\",\n",
    "     \"category\": \"semantic_similarity\", \"difficulty\": \"hard\",\n",
    "     \"expected_keywords\": [\"ramp\", \"velocity\", \"smooth\", \"jerk\", \"acceleration\"]},\n",
    "    {\"query\": \"Preventing the motor from losing track of its location\",\n",
    "     \"category\": \"semantic_similarity\", \"difficulty\": \"hard\",\n",
    "     \"expected_keywords\": [\"position\", \"encoder\", \"closed-loop\", \"stall\", \"deviation\"]},\n",
    "    {\"query\": \"Connecting the chip to a microcontroller\",\n",
    "     \"category\": \"semantic_similarity\", \"difficulty\": \"medium\",\n",
    "     \"expected_keywords\": [\"SPI\", \"interface\", \"communication\", \"microcontroller\", \"connection\"]},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデル定義\n",
    "\n",
    "CPU版の6モデル + GPU向け大型モデルを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    # --- CPU版と同じ6モデル（GPU上での速度比較用） ---\n",
    "    {\"name\": \"all-MiniLM-L6-v2\",\n",
    "     \"dim\": 384, \"params\": \"22M\", \"type\": \"small\",\n",
    "     \"description\": \"軽量ベースライン\", \"prefix\": None},\n",
    "    {\"name\": \"all-mpnet-base-v2\",\n",
    "     \"dim\": 768, \"params\": \"110M\", \"type\": \"base\",\n",
    "     \"description\": \"SentenceTransformers最高品質\", \"prefix\": None},\n",
    "    {\"name\": \"BAAI/bge-small-en-v1.5\",\n",
    "     \"dim\": 384, \"params\": \"33M\", \"type\": \"small\",\n",
    "     \"description\": \"BAAI小型\", \"prefix\": \"Represent this sentence: \"},\n",
    "    {\"name\": \"intfloat/e5-small-v2\",\n",
    "     \"dim\": 384, \"params\": \"33M\", \"type\": \"small\",\n",
    "     \"description\": \"Microsoft E5小型\", \"prefix\": \"query: \"},\n",
    "    {\"name\": \"thenlper/gte-small\",\n",
    "     \"dim\": 384, \"params\": \"33M\", \"type\": \"small\",\n",
    "     \"description\": \"Alibaba GTE小型\", \"prefix\": None},\n",
    "\n",
    "    # --- GPU向け大型モデル（CPUでは遅すぎたモデル） ---\n",
    "    {\"name\": \"BAAI/bge-base-en-v1.5\",\n",
    "     \"dim\": 768, \"params\": \"110M\", \"type\": \"base\",\n",
    "     \"description\": \"BAAI base。MTEBトップクラス\", \"prefix\": \"Represent this sentence: \"},\n",
    "    {\"name\": \"BAAI/bge-large-en-v1.5\",\n",
    "     \"dim\": 1024, \"params\": \"335M\", \"type\": \"large\",\n",
    "     \"description\": \"BAAI large。最高精度クラス\", \"prefix\": \"Represent this sentence: \"},\n",
    "    {\"name\": \"intfloat/e5-base-v2\",\n",
    "     \"dim\": 768, \"params\": \"110M\", \"type\": \"base\",\n",
    "     \"description\": \"Microsoft E5 base\", \"prefix\": \"query: \"},\n",
    "    {\"name\": \"intfloat/e5-large-v2\",\n",
    "     \"dim\": 1024, \"params\": \"335M\", \"type\": \"large\",\n",
    "     \"description\": \"Microsoft E5 large\", \"prefix\": \"query: \"},\n",
    "    {\"name\": \"thenlper/gte-base\",\n",
    "     \"dim\": 768, \"params\": \"110M\", \"type\": \"base\",\n",
    "     \"description\": \"Alibaba GTE base\", \"prefix\": None},\n",
    "    {\"name\": \"thenlper/gte-large\",\n",
    "     \"dim\": 1024, \"params\": \"335M\", \"type\": \"large\",\n",
    "     \"description\": \"Alibaba GTE large\", \"prefix\": None},\n",
    "]\n",
    "\n",
    "ALPHA_VALUES = [0.0, 0.5, 0.7, 1.0]\n",
    "\n",
    "print(f\"評価モデル数: {len(MODELS)}\")\n",
    "for m in MODELS:\n",
    "    print(f\"  {m['name']:<35} {m['params']:>5}  {m['dim']:>5}d  {m['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PDF処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PDFProcessor(chunk_size=512, chunk_overlap=50)\n",
    "chunks = processor.process_pdf(PDF_PATH)\n",
    "print(f\"\\nChunk examples:\")\n",
    "for c in chunks[:2]:\n",
    "    print(f\"  [{c['id']}] {c['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 評価実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "\n",
    "def evaluate_single_model(model_info, chunks, device=\"cuda\"):\n",
    "    model_name = model_info[\"name\"]\n",
    "    prefix = model_info.get(\"prefix\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating: {model_name} ({model_info['params']}, {model_info['dim']}d)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # 初期化\n",
    "    init_start = time.time()\n",
    "    try:\n",
    "        rag = HybridRAGSystem(\n",
    "            collection_name=f\"eval_{model_name.replace('/', '_').replace('-','_')}\",\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        return None\n",
    "    init_time = time.time() - init_start\n",
    "\n",
    "    # インデキシング\n",
    "    add_start = time.time()\n",
    "    rag.add_documents(chunks, batch_size=128)\n",
    "    add_time = time.time() - add_start\n",
    "\n",
    "    print(f\"  Init: {init_time:.2f}s | Indexing: {add_time:.2f}s\")\n",
    "\n",
    "    # GPU メモリ\n",
    "    gpu_mem_mb = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    model_results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"dim\": model_info[\"dim\"],\n",
    "        \"params\": model_info[\"params\"],\n",
    "        \"type\": model_info[\"type\"],\n",
    "        \"description\": model_info[\"description\"],\n",
    "        \"device\": device,\n",
    "        \"init_time\": round(init_time, 2),\n",
    "        \"indexing_time\": round(add_time, 2),\n",
    "        \"gpu_memory_mb\": round(gpu_mem_mb, 1),\n",
    "        \"alpha_results\": {},\n",
    "    }\n",
    "\n",
    "    for alpha in ALPHA_VALUES:\n",
    "        query_results = []\n",
    "        for test in TEST_CASES:\n",
    "            query = test[\"query\"]\n",
    "            q_for_dense = (prefix + query) if prefix else query\n",
    "\n",
    "            search_start = time.time()\n",
    "            if alpha == 1.0:\n",
    "                raw = rag.dense_search(q_for_dense, top_k=5)\n",
    "                formatted = [\n",
    "                    {\"id\": r[0], \"score\": r[1], \"text\": r[2],\n",
    "                     \"dense_score\": r[1], \"sparse_score\": 0.0}\n",
    "                    for r in raw\n",
    "                ]\n",
    "            elif alpha == 0.0:\n",
    "                formatted = rag.hybrid_search(query, alpha=0.0, top_k=5)\n",
    "            else:\n",
    "                formatted = rag.hybrid_search(q_for_dense, alpha=alpha, top_k=5)\n",
    "            search_time = time.time() - search_start\n",
    "\n",
    "            keyword_matches = []\n",
    "            for result in formatted[:3]:\n",
    "                text_lower = result[\"text\"].lower()\n",
    "                matches = sum(1 for kw in test[\"expected_keywords\"] if kw.lower() in text_lower)\n",
    "                keyword_matches.append(matches / len(test[\"expected_keywords\"]))\n",
    "            avg_kw = sum(keyword_matches) / len(keyword_matches) if keyword_matches else 0\n",
    "            top_score = formatted[0][\"score\"] if formatted else 0\n",
    "\n",
    "            query_results.append({\n",
    "                \"query\": test[\"query\"],\n",
    "                \"category\": test[\"category\"],\n",
    "                \"difficulty\": test[\"difficulty\"],\n",
    "                \"search_time\": round(search_time, 4),\n",
    "                \"keyword_match_score\": round(avg_kw, 4),\n",
    "                \"top_score\": round(top_score, 4),\n",
    "                \"top_text_preview\": formatted[0][\"text\"][:100] if formatted else \"\",\n",
    "            })\n",
    "\n",
    "        avg_st = np.mean([r[\"search_time\"] for r in query_results])\n",
    "        avg_kw = np.mean([r[\"keyword_match_score\"] for r in query_results])\n",
    "        avg_ts = np.mean([r[\"top_score\"] for r in query_results])\n",
    "\n",
    "        cat_scores = defaultdict(list)\n",
    "        for r in query_results:\n",
    "            cat_scores[r[\"category\"]].append(r[\"keyword_match_score\"])\n",
    "        cat_avg = {c: round(np.mean(s), 4) for c, s in cat_scores.items()}\n",
    "\n",
    "        model_results[\"alpha_results\"][f\"alpha_{alpha}\"] = {\n",
    "            \"avg_search_time\": round(avg_st, 4),\n",
    "            \"avg_keyword_match\": round(avg_kw, 4),\n",
    "            \"avg_top_score\": round(avg_ts, 4),\n",
    "            \"category_scores\": cat_avg,\n",
    "            \"query_details\": query_results,\n",
    "        }\n",
    "\n",
    "        print(f\"  alpha={alpha}: KW={avg_kw:.4f} TopScore={avg_ts:.4f} Time={avg_st:.4f}s\")\n",
    "\n",
    "    # GPU メモリ解放\n",
    "    del rag\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Models: {len(MODELS)} | Alpha values: {ALPHA_VALUES} | Test cases: {len(TEST_CASES)}\")\n",
    "print(f\"Total evaluations: {len(MODELS) * len(ALPHA_VALUES) * len(TEST_CASES)}\\n\")\n",
    "\n",
    "all_results = []\n",
    "failed = []\n",
    "total_start = time.time()\n",
    "\n",
    "for i, mi in enumerate(MODELS):\n",
    "    print(f\"\\n[{i+1}/{len(MODELS)}] {mi['name']}\")\n",
    "    try:\n",
    "        r = evaluate_single_model(mi, chunks, device=device)\n",
    "        if r:\n",
    "            all_results.append(r)\n",
    "        else:\n",
    "            failed.append(mi[\"name\"])\n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {e}\")\n",
    "        traceback.print_exc()\n",
    "        failed.append(mi[\"name\"])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"DONE: {len(all_results)} succeeded, {len(failed)} failed in {total_time:.1f}s\")\n",
    "if failed:\n",
    "    print(f\"Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 結果サマリー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha=0.7 でのランキング\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"RANKING (alpha=0.7) - sorted by Keyword Match Score\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"{'#':<3} {'Model':<35} {'Params':>6} {'Dim':>5} {'KW Match':>9} {'TopScore':>9} {'Index(s)':>9} {'Search(s)':>10}\")\n",
    "print(f\"{'-'*90}\")\n",
    "\n",
    "ranked = sorted(all_results,\n",
    "                key=lambda x: x[\"alpha_results\"].get(\"alpha_0.7\", {}).get(\"avg_keyword_match\", 0),\n",
    "                reverse=True)\n",
    "\n",
    "for rank, r in enumerate(ranked, 1):\n",
    "    a = r[\"alpha_results\"].get(\"alpha_0.7\", {})\n",
    "    print(f\"{rank:<3} {r['model_name']:<35} {r['params']:>6} {r['dim']:>5} \"\n",
    "          f\"{a.get('avg_keyword_match',0):>9.4f} {a.get('avg_top_score',0):>9.4f} \"\n",
    "          f\"{r['indexing_time']:>9.2f} {a.get('avg_search_time',0):>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カテゴリ別ベスト (alpha=0.7)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST MODEL BY CATEGORY (alpha=0.7)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "cats = set()\n",
    "for r in all_results:\n",
    "    cats.update(r[\"alpha_results\"].get(\"alpha_0.7\", {}).get(\"category_scores\", {}).keys())\n",
    "\n",
    "for cat in sorted(cats):\n",
    "    best_name, best_score = \"\", -1\n",
    "    for r in all_results:\n",
    "        s = r[\"alpha_results\"].get(\"alpha_0.7\", {}).get(\"category_scores\", {}).get(cat, 0)\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_name = r[\"model_name\"]\n",
    "    print(f\"  {cat:<25} -> {best_name:<35} ({best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha別の最良モデル\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BEST MODEL BY ALPHA VALUE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for alpha in ALPHA_VALUES:\n",
    "    key = f\"alpha_{alpha}\"\n",
    "    best_name, best_score = \"\", -1\n",
    "    for r in all_results:\n",
    "        s = r[\"alpha_results\"].get(key, {}).get(\"avg_keyword_match\", 0)\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_name = r[\"model_name\"]\n",
    "    print(f\"  alpha={alpha}: {best_name:<35} (KW Match: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズ別比較（small vs base vs large）\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SIZE CLASS COMPARISON (alpha=0.7)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for size_class in [\"small\", \"base\", \"large\"]:\n",
    "    models_in_class = [r for r in all_results if r[\"type\"] == size_class]\n",
    "    if not models_in_class:\n",
    "        continue\n",
    "    print(f\"\\n  [{size_class.upper()}]\")\n",
    "    for r in sorted(models_in_class,\n",
    "                    key=lambda x: x[\"alpha_results\"].get(\"alpha_0.7\", {}).get(\"avg_keyword_match\", 0),\n",
    "                    reverse=True):\n",
    "        a = r[\"alpha_results\"].get(\"alpha_0.7\", {})\n",
    "        print(f\"    {r['model_name']:<35} KW={a.get('avg_keyword_match',0):.4f}  \"\n",
    "              f\"Index={r['indexing_time']:.1f}s  GPU={r.get('gpu_memory_mb',0):.0f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 結果保存 & ダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"evaluation_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"device\": device,\n",
    "    \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "    \"num_chunks\": len(chunks),\n",
    "    \"num_test_cases\": len(TEST_CASES),\n",
    "    \"alpha_values\": ALPHA_VALUES,\n",
    "    \"total_time_sec\": round(total_time, 1),\n",
    "    \"failed_models\": failed,\n",
    "    \"results\": all_results,\n",
    "}\n",
    "\n",
    "output_path = \"gpu_model_comparison_results.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved: {output_path}\")\n",
    "print(f\"Total time: {total_time:.1f}s\")\n",
    "\n",
    "# ダウンロード\n",
    "files.download(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CPU vs GPU 速度比較（オプション）\n",
    "\n",
    "ベースラインモデルでCPU/GPU両方測定し、GPU高速化の効果を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPU 速度比較（all-MiniLM-L6-v2 のみ）\n",
    "if torch.cuda.is_available():\n",
    "    test_model = {\"name\": \"all-MiniLM-L6-v2\", \"dim\": 384, \"params\": \"22M\",\n",
    "                  \"type\": \"small\", \"description\": \"baseline\", \"prefix\": None}\n",
    "\n",
    "    print(\"CPU vs GPU Speed Comparison (all-MiniLM-L6-v2)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for dev in [\"cpu\", \"cuda\"]:\n",
    "        r = evaluate_single_model(test_model, chunks, device=dev)\n",
    "        if r:\n",
    "            a07 = r[\"alpha_results\"].get(\"alpha_0.7\", {})\n",
    "            print(f\"\\n  {dev.upper()}: Init={r['init_time']:.1f}s  \"\n",
    "                  f\"Index={r['indexing_time']:.1f}s  \"\n",
    "                  f\"Search={a07.get('avg_search_time',0):.4f}s/query\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping comparison\")"
   ]
  }
 ]
}